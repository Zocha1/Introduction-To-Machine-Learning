{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPfmBWXcrnoU5xRMHEAyR5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Zocha1/Introduction-To-Machine-Learning/blob/main/4_PRZYK%C5%81AD_Przetwarzanie_tekstu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Ładowanie zbioru danych"
      ],
      "metadata": {
        "id": "RAyQjUxRlEpS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPzZ280RwuT6"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://keras.io/api/datasets/imdb/\n",
        "\n",
        "**'num_words=10000'** - użyjemy 10.000 najczęściej występujących słów\n",
        "\n",
        "**'train_lables' 'test_labels'** - zmienne zawierające etykiety: 0 oznaczających recenzję negatywną i 1 oznaczających recenzję pozytywną\n",
        "\n"
      ],
      "metadata": {
        "id": "uOqC_zMNycnX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[0]"
      ],
      "metadata": {
        "id": "IUgYfscmyWy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[1]"
      ],
      "metadata": {
        "id": "eRQaiIfpzUbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "2Fpuba8w3eXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test_data)"
      ],
      "metadata": {
        "id": "E5-ghSwS33XM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_review = ' '.join([reverse_word_index.get(i-3,'?') for i in train_data[1]])\n",
        "\n",
        "print(decoded_review)"
      ],
      "metadata": {
        "id": "2hFDkQF55_O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Przygotowanie danych\n",
        "Do sieci neuronowej nie można wczytać list liczb całkowitych, trzeba je zamienić na listę **tensorów**. Istnieją dwa sposoby, które nam to umożliwią:\n",
        "- Należy dopełnić listy, aby wszystkie były jednakowej długości i wtedy zamienić je na **tensor liczb całkowitych** o kształcie: (*próbki, indeksy_słów*). Wtedy pierwszą warstą sieci neuronowej musi być warstwa **Embedding**, która jest w stanie przetwarzać tensory liczb całkowitych.\n",
        "- **One-hot encoding** - listy można także zamienić na wektory 0 i 1. Na przykład, jeśli twoja sekwencja to [3, 5], a masz słownik o rozmiarze 10 000 słów, to zapiszesz tę sekwencję jako wektor o długości 10 000, gdzie wszystkie elementy są zerami, oprócz indeksów 3 i 5, które są ustawione na 1. Wykorzystując tą metodę pierwszą warstwą sieci może być warstwa Dense, która jest w stanie obsługiwać wektory danych zmiennoprzecinkowych.\n",
        "\n",
        "Korzystamy z drugiego rozwiązania:\n",
        "\n",
        "Tworzymy tensor 2D, czyli macierz o wymiarach 25.000 x 10.000 wypełnioną 0 i 1. 25.000, bo tyle mamy recenzji i 10.000, bo tyle jest możliwych słów w recenzjach.\n",
        "Przykładowo, jeśli recenzja składa się z trzech słów, które w naszym ograniczonym słowniku mają indeksy 5, 8 i 10, wówczas w wierszu odpowiadającym tej recenzji, w kolumnach 5, 8, 10 zostaną umieszczone jedynki, a reszta pozostanie zerami.\n"
      ],
      "metadata": {
        "id": "_yywRV4mdokJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def vectorize_sequences(sequences, dimension = 10000):\n",
        "  results = np.zeros((len(sequences), dimension))\n",
        "  for i, sequence in enumerate(sequences):\n",
        "    results[i, sequence] = 1.\n",
        "  return results\n",
        "\n",
        "x_train = vectorize_sequences(train_data)\n",
        "x_test = vectorize_sequences(test_data)"
      ],
      "metadata": {
        "id": "Fb0rjEfg8T36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train[0]"
      ],
      "metadata": {
        "id": "tl1GuZi0-BQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Zanim przejdziemy do przetwarzania danych przez sieć neuronową, musimy jeszcze zamienić na wektory etykiety próbek."
      ],
      "metadata": {
        "id": "Gxo9xFoFkSOL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.asarray(train_labels).astype('float32')\n",
        "y_test = np.asarray(test_labels).astype('float32')"
      ],
      "metadata": {
        "id": "Zxb1Mdtc-PMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "MIJ3uTwI-fqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Po przetworzeniu danych, etykiety są wartościami skalarnymi - zera i jedynki, dane wejściowe wektorami."
      ],
      "metadata": {
        "id": "2pnmN6_nls8j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Budowa sieci neuronowej\n",
        "\n",
        "Głównym blokiem składowym sieci neuronowej jest **warstwa (layer)**. Można ją traktować jako filtr danych. Dane wychodzące z filtra mają bardziej przydatną formę od danych wchodzących.\n",
        "\n",
        "Model uczenia głębokiego  jest jak sito przetwarzające dane składające się z coraz drobniejszych siatek-warstw.\n",
        "\n",
        "Do rozwiązania naszego problemy najlepiej zastosować sieci prostego stosu w pełni połączonych warst - **Dense**:\n",
        "- dwie pierwsze warsty z aktywacją - **relu**, zawierające 16 ukrytych jednostek (neuronów)\n",
        "- ostatnia warstwa definijąca przewidywania sentymentu, czy recenzja będzie pozytywna, czy negatywna z funkcją aktywacji - **sigmoid**, która reprezentuje wartości w zakresie od 0 do 1, co pozwalana na określenie prawdopodobieństwa, zawierająca 1 neuron, ponieważ dla klasyfikacji binarnej wynik jest pojedynczą wartością prawdopodobieństwa przynależności do klasy pozytywnej"
      ],
      "metadata": {
        "id": "_HjVhPIgk7Pd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "yLuTBAEZ_Dwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na tym etapie musimy określić jeszcze 3 rzeczy w celu przygotowania sieci do trenowania:\n",
        "- **funkcja straty**: sposób pomiaru wydajności sieci podczas przetwarzania treningowego zbioru danych - dostrajanie parametrów sieci we właściwym kierunku\n",
        "- **optymalizator**: mechanizm dostrajania sieci na podstawie danych zwracanych przez funkcję straty\n",
        "- **metryki monitorowania podczas trenowania i testowania**: dokładność, czyli część obrazków, która została dobrze sklasyfikowana"
      ],
      "metadata": {
        "id": "PyOH7MfLIsar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Nasz problem polega na klasyfikacji binarnej, w której sieć zwraca nam wartość prawdopodobieństwa, dlatego najlepszym wyborem funkcji straty będzie binarna entropia krzyżowa - **binary_crossentropy**.\n",
        "- Dobrym wyborem do więkoszści problemów będzie optymalizator **rmsprop**.\n",
        "- Metryką do zadań klasyfikacji binarnej jest dokładność - **accuracy**"
      ],
      "metadata": {
        "id": "8oTXv-p9p-Od"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "guegdILv_8Nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Walidacja modelu\n",
        "\n",
        "\n",
        "Walidacja modelu jest kluczowym krokiem w procesie budowania i optymalizacji modeli uczenia maszynowego, w tym sieci neuronowych. Jej głównym celem jest ocena, jak dobrze model generalizuje się na nowych, nieznanych danych, które nie były wykorzystywane podczas treningu.\n",
        "\n",
        "W tym celu utworzymy zbiór danych, które nie były wykorzystane podczas trenowania modelu. Zrobimy to przez oddzielenie 10.000 próbek od treningowego zbioru danych.\n"
      ],
      "metadata": {
        "id": "jIyr15uDw0uS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = x_train[:10000]\n",
        "partial_x_train = x_train[10000:]\n",
        "\n",
        "y_val = y_train[:10000]\n",
        "partial_y_train = y_train[10000:]"
      ],
      "metadata": {
        "id": "BnlTLFxAAOME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Trenowanie modelu\n",
        "\n",
        "Teraz będziemy trenować model przez **20 epok**. To oznacza, że cały zbiór treningowy x_train i odpowiadające mu etykiety y_train zostaną przepuszczone przez model 20 razy, co pozwala na dokładniejsze dostosowanie wag modelu do przetwarzanych danych.\n",
        "\n",
        "Z podziałem na **wsady po 512 próbek**. Oznacza to, że zamiast aktualizować wagi modelu po każdej próbce (co jest nieefektywne) lub po całym zestawie danych (co może wymagać dużo pamięci i być wolne), aktualizacje wag są przeprowadzane po każdych 512 próbkach. To ułatwia efektywniejsze i szybsze treningi.\n",
        "\n",
        "Jednocześnie będziemy monitorować funkcje straty i dokładność modelu przy przetwarzaniu 10.000 próbek, które przed chwilą wyodrębniliśmy. W tym celu musimy przekazać zbiór walidacyjny (kontrolny) jako argument validation_data.\n",
        "Pod koniec trwania każdej epoki algorytm zatrzymuje się na chwilę, ponieważ model oblicza stratę i dokładność, korzystając z 10.000 próbek walidacyjnego zbioru danych.\n",
        "\n"
      ],
      "metadata": {
        "id": "K-Ut2BLnxu5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(partial_x_train,\n",
        "                    partial_y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "M6E4LhFyAkuE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Wywołanie metody **model.fit()** zwraca obiekte **History**, zawierający element **history** - słownik danych dotyczących przebiegu trenowania."
      ],
      "metadata": {
        "id": "_0-KwL3tziBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history_dict = history.history\n",
        "history_dict.keys()"
      ],
      "metadata": {
        "id": "-ZkOzyCwBNTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ocena wydajności już wytrenowanego modelu na zestawie danych testowych, pozwala na sprawdzenie, jak dobrze model generalizuje się na nowych, niewidzianych wcześniej danych."
      ],
      "metadata": {
        "id": "S0Z23Sgbs1ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "LxAP-_HJBXQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Interpretacja wyników przewidywań"
      ],
      "metadata": {
        "id": "ee2R3LwOp6cK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "loss_values = history_dict['loss']\n",
        "val_loss_values = history_dict['val_loss']\n",
        "\n",
        "epochs = range(1, len(loss_values) + 1)\n",
        "\n",
        "plt.plot(epochs, loss_values, 'bo', label='Strata trenowania')\n",
        "plt.plot(epochs, val_loss_values, 'b', label='Strata walidacji')\n",
        "plt.title('Strata trenowania i walidacji')\n",
        "plt.xlabel('Epoki')\n",
        "plt.ylabel('Strata')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kAobGd0XJAsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.clf()\n",
        "\n",
        "acc_values = history_dict['acc']\n",
        "val_acc_values = history_dict['val_acc']\n",
        "\n",
        "plt.plot(epochs, acc_values, 'bo', label='Dokładność trenowania')\n",
        "plt.plot(epochs, val_acc_values, 'b', label='Dokładność walidacji')\n",
        "plt.title('Dokładność trenowania i walidacji')\n",
        "plt.xlabel('Epoki')\n",
        "plt.ylabel('Dokładność')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-s3Hn6SqKzXe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}